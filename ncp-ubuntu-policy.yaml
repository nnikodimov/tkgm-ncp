

apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: nsxerrors.nsx.vmware.com
spec:
  group: nsx.vmware.com
  versions:
    - name: v1
      served: true
      storage: true

      schema:
        openAPIV3Schema:
          type: object
          x-kubernetes-preserve-unknown-fields: true
      additionalPrinterColumns:
      - name: Messages
        type: string
        description: NSX error messages. Messages are sorted by timestamp on which the error occurs.
        jsonPath: .spec.message
      - name: ErrorObjectID
        type: string
        description: The identifier of the k8s object which has the errors.
        jsonPath: .spec.error-object-id
      - name: ErrorObjectType
        type: string
        description: The type of the k8s object which has the errors.
        jsonPath: .spec.error-object-type
      - name: ErrorObjectName
        type: string
        description: The name of the k8s object which has the errors.
        jsonPath: .spec.error-object-name
      - name: ErrorObjectNamespace
        type: string
        description: The namespace of the k8s object if it is namespaced. None by default
        jsonPath: .spec.error-object-ns

  scope: Cluster
  names:
    plural: nsxerrors
    singular: nsxerror
    kind: NSXError
    shortNames:
    - ne


---
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: loadbalancers.vmware.com
spec:
  group: vmware.com
  versions:
    - name: v1alpha1
      served: true
      storage: true

      schema:
        openAPIV3Schema:
          type: object
          x-kubernetes-preserve-unknown-fields: true

  scope: Namespaced
  names:
    plural: loadbalancers
    singular: loadbalancer
    kind: LoadBalancer
    shortNames:
    - lb
---
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: nsxlbmonitors.vmware.com
spec:
  group: vmware.com
  versions:
    - name: v1alpha1
      served: true
      storage: true

      schema:
        openAPIV3Schema:
          type: object
          x-kubernetes-preserve-unknown-fields: true

  scope: Namespaced
  names:
    plural: nsxlbmonitors
    singular: nsxlbmonitor
    kind: NSXLoadBalancerMonitor
    shortNames:
    - lbm

---
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: nsxlocks.nsx.vmware.com
spec:
  group: nsx.vmware.com
  versions:
    - name: v1
      served: true
      storage: true

      schema:
        openAPIV3Schema:
          type: object
          x-kubernetes-preserve-unknown-fields: true

  scope: Cluster
  names:
    plural: nsxlocks
    singular: nsxlock
    kind: NSXLock
    shortNames:
    - nsxlo
---
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: ncpconfigs.nsx.vmware.com
spec:
  group: nsx.vmware.com
  versions:
    - name: v1
      served: true
      storage: true

      schema:
        openAPIV3Schema:
          type: object
          x-kubernetes-preserve-unknown-fields: true

  scope: Cluster
  names:
    plural: ncpconfigs
    singular: ncpconfig
    kind: NCPConfig
    shortNames:
    - ncpcfg

---
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: virtualnetworks.vmware.com
spec:
  group: vmware.com
  versions:
    - name: v1alpha1
      served: true
      storage: true

      schema:
        openAPIV3Schema:
          type: object
          x-kubernetes-preserve-unknown-fields: true

  scope: Namespaced
  names:
    plural: virtualnetworks
    singular: virtualnetwork
    kind: VirtualNetwork
    shortNames:
    - vnet
---
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: virtualnetworkinterfaces.vmware.com
spec:
  group: vmware.com
  versions:
    - name: v1alpha1
      served: true
      storage: true

      schema:
        openAPIV3Schema:
          type: object
          x-kubernetes-preserve-unknown-fields: true

  scope: Namespaced
  names:
    plural: virtualnetworkinterfaces
    singular: virtualnetworkinterface
    kind: VirtualNetworkInterface
    shortNames:
    - vnetif
---
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: network-attachment-definitions.k8s.cni.cncf.io
spec:
  group: k8s.cni.cncf.io
  versions:
    - name: v1
      served: true
      storage: true

      schema:
        openAPIV3Schema:
          type: object
          x-kubernetes-preserve-unknown-fields: true

  scope: Namespaced
  names:
    plural: network-attachment-definitions
    singular: network-attachment-definition
    kind: NetworkAttachmentDefinition
    shortNames:
    - net-attach-def

---
# Create Namespace for NSX owned resources
kind: Namespace
apiVersion: v1
metadata:
 name: nsx-system


---

# Create a ServiceAccount for nsx-node-agent
apiVersion: v1
kind: ServiceAccount
metadata:
 name: nsx-node-agent-svc-account
 namespace: nsx-system
---

# Create ClusterRole for nsx-node-agent
kind: ClusterRole
# Set the apiVersion to rbac.authorization.k8s.io/v1beta1 when k8s < v1.8
apiVersion: rbac.authorization.k8s.io/v1
metadata:
 name: nsx-node-agent-cluster-role
rules:
 - apiGroups:
   - ""
   resources:
     - endpoints
     - services
   verbs:
     - get
     - watch
     - list
 - apiGroups:
   - ""
   resources:
     - pods
   verbs:
     - get
     - list
     - update
     - patch

---

# Bind ServiceAccount created for nsx-node-agent to its ClusterRole
kind: ClusterRoleBinding
# Set the apiVersion to rbac.authorization.k8s.io/v1beta1 when k8s < v1.8
apiVersion: rbac.authorization.k8s.io/v1
metadata:
 name: nsx-node-agent-cluster-role-binding
roleRef:

 apiGroup: rbac.authorization.k8s.io

 kind: ClusterRole
 name: nsx-node-agent-cluster-role
subjects:

 - kind: ServiceAccount
   name: nsx-node-agent-svc-account
   namespace: nsx-system

---

# Create a ServiceAccount for NCP namespace
apiVersion: v1
kind: ServiceAccount
metadata:
 name: ncp-svc-account
 namespace: nsx-system
---

# Create ClusterRole for NCP
kind: ClusterRole
# Set the apiVersion to rbac.authorization.k8s.io/v1beta1 when k8s < v1.8
apiVersion: rbac.authorization.k8s.io/v1
metadata:
 name: ncp-cluster-role
rules:
 - apiGroups:
   - ""
   - extensions
   - networking.k8s.io

   resources:
     - deployments
     - endpoints
     - pods/log
     - nodes
     - replicationcontrollers
     # Remove 'secrets' if not using Native Load Balancer.
     - secrets
      # new resource for k8s 1.18
     - ingressclasses


   verbs:
     - get
     - watch
     - list

---

# Create ClusterRole for NCP to edit resources
kind: ClusterRole
# Set the apiVersion to rbac.authorization.k8s.io/v1beta1 when k8s < v1.8
apiVersion: rbac.authorization.k8s.io/v1
metadata:
 name: ncp-patch-role
rules:
 - apiGroups:
   - ""
   - extensions
   - networking.k8s.io
   resources:
     # NCP needs to annotate the SNAT errors on namespaces
     - namespaces
     - ingresses
     - services
     # NCP needs to annotate Pods for multus
     - pods
     # NCP needs to annotate Networkpolicies for validation and update errors
     - networkpolicies

   verbs:
     - get
     - watch
     - list
     - update
     - patch
 # NCP needs permission to CRUD custom resource nsxerrors
 - apiGroups:
   # The api group is specified in custom resource definition for nsxerrors
   - nsx.vmware.com
   resources:
     - nsxerrors

     - nsxlocks
     - ncpconfigs
   verbs:
     - create
     - get
     - list
     - patch
     - delete

     - update
 - apiGroups:
   - ""
   - extensions
   - networking.k8s.io

   resources:
     - ingresses/status
     - services/status


   verbs:
     - replace
     - update
     - patch

 - apiGroups:
   - vmware.com
   resources:
     - loadbalancers
     - loadbalancers/status
     - nsxlbmonitors
     - nsxlbmonitors/status
   
     - virtualnetworks
     - virtualnetworks/status
     - virtualnetworkinterfaces
     - virtualnetworkinterfaces/status
   
   verbs:
     - create
     - get
     - list
     - patch
     - delete
     - watch
     - update




 - apiGroups:
   - k8s.cni.cncf.io
   resources:
     - network-attachment-definitions
   verbs:
     - get
     - list
     - watch

---

# Bind ServiceAccount created for NCP to its ClusterRole
kind: ClusterRoleBinding
# Set the apiVersion to rbac.authorization.k8s.io/v1beta1 when k8s < v1.8
apiVersion: rbac.authorization.k8s.io/v1
metadata:
 name: ncp-cluster-role-binding
roleRef:

 # Comment out the apiGroup while using OpenShift
 apiGroup: rbac.authorization.k8s.io

 kind: ClusterRole
 name: ncp-cluster-role
subjects:

 - kind: ServiceAccount
   name: ncp-svc-account
   namespace: nsx-system


---

# Bind ServiceAccount created for NCP to the patch ClusterRole
kind: ClusterRoleBinding
# Set the apiVersion to rbac.authorization.k8s.io/v1beta1 when k8s < v1.8
apiVersion: rbac.authorization.k8s.io/v1
metadata:
 name: ncp-patch-role-binding
roleRef:

 apiGroup: rbac.authorization.k8s.io

 kind: ClusterRole
 name: ncp-patch-role
subjects:

 - kind: ServiceAccount
   name: ncp-svc-account
   namespace: nsx-system


---
apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: nsx-node-agent-psp
spec:
  hostIPC: false
  hostNetwork: true
  hostPID: true
  privileged: true
  allowedCapabilities:
  - SYS_ADMIN
  - NET_ADMIN
  - SYS_PTRACE
  - DAC_READ_SEARCH
  - SYS_NICE
  - SYS_MODULE
  defaultAddCapabilities: null
  fsGroup:
   rule: RunAsAny
  readOnlyRootFilesystem: false
  requiredDropCapabilities:
  - KILL
  runAsUser:
    rule: RunAsAny
  seLinux:
    rule: RunAsAny
    seLinuxOptions:
      type: spc_t
      level: s0:c0.c1023
  supplementalGroups:
    rule: RunAsAny
  volumes:
  - configMap
  - downwardAPI
  - emptyDir
  - persistentVolumeClaim
  - projected
  - secret
  - hostPath

---

kind: ClusterRole
# Set the apiVersion to rbac.authorization.k8s.io/v1beta1 when k8s < v1.8
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: nsx-node-agent-psp-cluster-role
rules:
- apiGroups:
  - policy
  resourceNames:
  - nsx-node-agent-psp
  resources:
  - podsecuritypolicies
  verbs:
  - use

---

kind: ClusterRoleBinding
# Set the apiVersion to rbac.authorization.k8s.io/v1beta1 when k8s < v1.8
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: nsx-node-agent-psp-cluster-role-binding
subjects:
- kind: ServiceAccount
  name: nsx-node-agent-svc-account
  namespace: nsx-system
roleRef:
  kind: ClusterRole
  name: nsx-node-agent-psp-cluster-role
  apiGroup: rbac.authorization.k8s.io

---

apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: ncp-psp
spec:
  hostNetwork: true
  hostIPC: false
  hostPID: false
  privileged: false
  defaultAddCapabilities: null
  requiredDropCapabilities:
  - KILL
  runAsUser:
    rule: RunAsAny
  volumes:
  - configMap
  - downwardAPI
  - emptyDir
  - persistentVolumeClaim
  - projected
  - secret
  seLinux:
    rule: RunAsAny
  supplementalGroups:
    rule: MustRunAs
    ranges:
      # Forbid adding the root group.
      - min: 1
        max: 65535
  fsGroup:
    rule: MustRunAs
    ranges:
      # Forbid adding the root group.
      - min: 1
        max: 65535
  readOnlyRootFilesystem: false

---

kind: ClusterRole
# Set the apiVersion to rbac.authorization.k8s.io/v1beta1 when k8s < v1.8
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: ncp-psp-cluster-role
rules:
- apiGroups:
  - policy
  resourceNames:
  - ncp-psp
  resources:
  - podsecuritypolicies
  verbs:
  - use

---

kind: ClusterRoleBinding
# Set the apiVersion to rbac.authorization.k8s.io/v1beta1 when k8s < v1.8
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: ncp-psp-cluster-role-binding
subjects:
- kind: ServiceAccount
  name: ncp-svc-account
  namespace: nsx-system
roleRef:
  kind: ClusterRole
  name: ncp-psp-cluster-role
  apiGroup: rbac.authorization.k8s.io

---

# Client certificate and key used for NSX authentication
#kind: Secret
#metadata:
#  name: nsx-secret
#  namespace: nsx-system
#type: kubernetes.io/tls
#apiVersion: v1
#data:
#  # Fill in the client cert and key if using cert based auth with NSX
#  tls.crt:
#  tls.key:
---
# Certificate and key used for TLS termination in HTTPS load balancing
kind: Secret
metadata:
  name: lb-secret
  namespace: nsx-system
type: kubernetes.io/tls
apiVersion: v1
data:
#  # Fill in the server cert and key for TLS termination
  tls.crt: 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
  tls.key: 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




---
# Yaml template for NCP Deployment
# Proper kubernetes API and NSX API parameters, and NCP Docker image
# must be specified.
# This yaml file is part of NCP 3.1.0 release.

# ConfigMap for ncp.ini
apiVersion: v1
kind: ConfigMap
metadata:
  name: nsx-ncp-config
  namespace: nsx-system
  labels:
    version: v1
data:
  ncp.ini: |
    

    [vc]

    # IpAddress or Hostname of VC
    #vc_endpoint = <None>

    # The SSO domain associated with the deployment
    #sso_domain = vsphere.local

    # VC API server HTTPS port.
    #https_port = 443


    [coe]

    # Container orchestrator adaptor to plug in.
    #adaptor = kubernetes

    # Specify cluster for adaptor.
    cluster = sdc-tkg-service

    # Log level for NCP modules (controllers, services, etc.). Ignored if debug
    # is True
    # Choices: NOTSET DEBUG INFO WARNING ERROR CRITICAL
    loglevel = INFO

    # Log level for NSX API client operations. Ignored if debug is True
    # Choices: NOTSET DEBUG INFO WARNING ERROR CRITICAL
    #nsxlib_loglevel = <None>

    # Enable SNAT for all projects in this cluster. Modification of topologies
    # for existing Namespaces is not supported if this option is reset.
    enable_snat = True

    # Option to enable profiling
    #profiling = False

    # The interval of reporting performance metrics (0 means disabled)
    #metrics_interval = 0

    # Name of log file for outputting metrics only (if not defined, use default
    # logging facility)
    #metrics_log_file = <None>

    # The type of container host node
    # Choices: HOSTVM BAREMETAL CLOUD WCP_WORKER
    #node_type = HOSTVM

    # The time in seconds for NCP/nsx_node_agent to recover the connection to
    # NSX manager/container orchestrator adaptor/Hyperbus before exiting. If
    # the value is 0, NCP/nsx_node_agent won't exit automatically when the
    # connection check fails
    #connect_retry_timeout = 0


    # Enable system health status report for SHA
    #enable_sha = True


    [DEFAULT]

    # If set to true, the logging level will be set to DEBUG instead of the
    # default INFO level.
    debug = True

    # If set to true, log output to standard error.
    #use_stderr = True

    # Destination to send api log to. STDOUT or STDERR for console output. FILE
    # to write log to file configured in "api_log_file". NONE to disable api
    # log.
    # Choices: STDOUT STDERR FILE NONE
    #api_log_output = NONE

    # Name of log file to send API access log to.
    #api_log_file = ncp_api_log.txt

    # Interval in seconds to logs api call to output configured in
    # api_log_output
    #api_log_interval = 60

    # When api_log_output is not NONE, this option determines if api calls
    # should be collected per NSX cluster or individual NSX manager.
    # Choices: API_LOG_PER_ENDPOINT API_LOG_PER_CLUSTER
    #api_log_mode = API_LOG_PER_ENDPOINT

    # If set to true, use syslog for logging.
    #use_syslog = False

    # The base directory used for relative log_file paths.
    #log_dir = <None>

    # Name of log file to send logging output to.
    #log_file = <None>

    # max MB for each compressed file. Defaults to 100 MB.
    #log_rotation_file_max_mb = 100

    # max MB for each compressed file for API logs.Defaults to 10 MB.
    #api_log_rotation_file_max_mb = 10

    # Total number of compressed backup files to store. Defaults to 5.
    #log_rotation_backup_count = 5

    # Total number of compressed backup files to store API logs. Defaults to 5.
    #api_log_rotation_backup_count = 5

    # Log level for the root logger. If debug=True, the default root logger
    # level will be DEBUG regardless of the value of this option. If this
    # option is unset, the default root logger level will be either DEBUG or
    # INFO according to the debug option value
    # Choices: NOTSET DEBUG INFO WARNING ERROR CRITICAL
    loglevel = INFO




    [nsx_v3]

    # Set NSX API adaptor to NSX Policy API adaptor. If unset, NSX adaptor will
    # be set to the NSX Manager based adaptor. If unset or False, the NSX
    # resource ID in other options can be resource name or UUID
    policy_nsxapi = True



    # Path to NSX client certificate file. If specified, the nsx_api_user and
    # nsx_api_password options will be ignored. Must be specified along with
    # nsx_api_private_key_file option
    #nsx_api_cert_file = <None>

    # Path to NSX client private key file. If specified, the nsx_api_user and
    # nsx_api_password options will be ignored. Must be specified along with
    # nsx_api_cert_file option
    #nsx_api_private_key_file = <None>

    # IP address of one or more NSX managers separated by commas. The IP
    # address should be of the form:
    # [<scheme>://]<ip_adress>[:<port>]
    # If scheme is not provided https is used. If port is not provided port 80
    # is used for http and port 443 for https.
    #nsx_api_managers = []
    nsx_api_managers = 192.168.110.15
    nsx_api_user = admin
    nsx_api_password = VMware1!VMware1!
    insecure = True


    # If True, skip fatal errors when no endpoint in the NSX management cluster
    # is available to serve a request, and retry the request instead
    #cluster_unavailable_retry = False

    # Maximum number of times to retry API requests upon stale revision errors.
    #retries = 10

    # Specify one or a list of CA bundle files to use in verifying the NSX
    # Manager server certificate. This option is ignored if "insecure" is set
    # to True. If "insecure" is set to False and "ca_file" is unset, the
    # "thumbprint" will be used. If "thumbprint" is unset, the system root CAs
    # will be used to verify the server certificate.
    #ca_file = []

    # Specify one or a list of thumbprint strings to use in verifying the NSX
    # Manager server certificate. This option is ignored if "insecure" is set
    # to True or "ca_file" is defined.
    #thumbprint = []

    # If true, the NSX Manager server certificate is not verified. If false the
    # CA bundle specified via "ca_file" will be used or if unset the
    # "thumbprint" will be used. If "thumbprint" is unset, the default system
    # root CAs will be used.
    #insecure = False

    # The time in seconds before aborting a HTTP connection to a NSX manager.
    #http_timeout = 10

    # The time in seconds before aborting a HTTP read response from a NSX
    # manager.
    #http_read_timeout = 180

    # Maximum number of times to retry a HTTP connection.
    #http_retries = 3

    # Maximum concurrent connections to all NSX managers. If multiple NSX
    # managers are configured, connections will be spread evenly across all
    # managers, rounded down to the nearest integer. Each NSX manager will have
    # at least 1 connection. This value should be a multiple of
    # [nsx_v3]nsx_api_managers length.
    #concurrent_connections = 10

    # The amount of time in seconds to wait before ensuring connectivity to the
    # NSX manager if no manager connection has been used.
    #conn_idle_timeout = 10

    # Number of times a HTTP redirect should be followed.
    #redirects = 2

    # Subnet prefix of IP block.
    subnet_prefix = 27

    # Subnet prefix for v6 IP blocks
    #v6_subnet_prefix = 64


    # Indicates whether distributed firewall DENY rules are logged.
    #log_dropped_traffic = False

    # Indicates whether distributed firewall rules are logged. Option 'ALL'
    # will enable logging for all DFW rules (both DENY and ALLOW), and option
    # 'DENY' will enable logging only for DENY rules. Remove this config if no
    # logging is desired
    # Choices: ALL DENY <None>
    #log_firewall_traffic = <None>


    # Option to use native load balancer or not
    use_native_loadbalancer = True


    # Option to auto scale layer 4 load balancer or not. If set to True, NCP
    # will create additional LB when necessary upon K8s Service of type LB
    # creation/update.
    l4_lb_auto_scaling = True

    # Option to use native load balancer or not when ingress class annotation
    # is missing. Only effective if use_native_loadbalancer is set to true
    #default_ingress_class_nsx = True

    # Path to the default certificate file for HTTPS load balancing. Must be
    # specified along with lb_priv_key_path option
    lb_default_cert_path = /etc/nsx-ujo/lb-cert/tls.crt

    # Path to the private key file for default certificate for HTTPS load
    # balancing. Must be specified along with lb_default_cert_path option
    lb_priv_key_path = /etc/nsx-ujo/lb-cert/tls.key

    # Option to set load balancing algorithm in load balancer pool object.
    # Choices: ROUND_ROBIN LEAST_CONNECTION IP_HASH WEIGHTED_ROUND_ROBIN
    pool_algorithm = LEAST_CONNECTION

    # Option to set load balancer service size. MEDIUM Edge VM (4 vCPU, 8GB)
    # only supports SMALL LB. LARGE Edge VM (8 vCPU, 16GB) only supports MEDIUM
    # and SMALL LB. Bare Metal Edge (IvyBridge, 2 socket, 128GB) supports
    # LARGE, MEDIUM and SMALL LB
    # Choices: SMALL MEDIUM LARGE
    service_size = SMALL

    # Option to set load balancer persistence option. If cookie is selected,
    # cookie persistence will be offered.If source_ip is selected, source IP
    # persistence will be offered for ingress traffic through L7 load balancer
    # Choices: <None> cookie source_ip
    #l7_persistence = <None>

    # An integer for LoadBalancer side timeout value in seconds on layer 7
    # persistence profile, if the profile exists.
    #l7_persistence_timeout = 10800

    # Option to set load balancer persistence option. If source_ip is selected,
    # source IP persistence will be offered for ingress traffic through L4 load
    # balancer
    # Choices: <None> source_ip
    #l4_persistence = <None>

    # Option to set distributed load balancer source ip persistence option,
    # only available when use_native_dlb = True
    # Choices: <None> source_ip
    #dlb_l4_persistence = <None>





    # Resource ID of the container ip blocks that will be used for creating
    # subnets. If name, it must be unique. If policy_nsxapi is enabled, it also
    # support automatically creating the IP blocks. The definition is a comma
    # separated list: CIDR,CIDR,... Mixing different formats (e.g. UUID,CIDR)
    # is not supported.
    container_ip_blocks = 10.174.16.0/20

    # Resource ID of the container ip blocks that will be used for creating
    # subnets for no-SNAT projects. If specified, no-SNAT projects will use
    # these ip blocks ONLY. Otherwise they will use container_ip_blocks
    no_snat_ip_blocks = 10.221.16.0/20

    # Resource ID of the external ip pools that will be used for allocating IP
    # addresses which will be used for translating container IPs via SNAT
    # rules. If policy_nsxapi is enabled, it also support automatically
    # creating the ip pools. The definition is a comma separated list:
    # CIDR,IP_1-IP_2,... Mixing different formats (e.g. UUID, CIDR&IP_Range) is
    # not supported.
    external_ip_pools = 10.221.96.0/23


    # Resource ID of the top-tier router for the container cluster network,
    # which could be either tier0 or tier1. If policy_nsxapi is enabled, should
    # be ID of a tier0/tier1 gateway.
    top_tier_router = sdc-tkg-service-t1

    # Option to use single-tier router for the container cluster network
    single_tier_topology = True

    # Option to use single-tier router for the container cluster network. Each
    # namespace will have dedicated tier-1 router created. Namespaces with
    # "sr_shared_res: true" annotation will share t1 and lbs.
    #single_tier_sr_topology = True

    # Resource ID of the external ip pools that will be used only for
    # allocating IP addresses for Ingress controller and LB service. If
    # policy_nsxapi is enabled, it also supports automatically creating the ip
    # pools. The definition is a comma separated list: CIDR,IP_1-IP_2,...
    # Mixing different formats (e.g. UUID, CIDR&IP_Range) is not supported.
    external_ip_pools_lb = 10.221.98.0/23

    # Resource ID of the NSX overlay transport zone that will be used for
    # creating logical switches for container networking. It must refer to an
    # already existing resource on NSX and every transport node where VMs
    # hosting containers are deployed must be enabled on this transport zone
    overlay_tz = 1b3a2f36-bfd1-443e-a0f6-4de01abc963e

    # Name of the enforcement point used to look up overlay transport zones and
    # edge cluster paths, e.g. vmc-enforcementpoint, default, etc.
    #enforcement_point = <None>

    # Resource ID of the lb service that can be attached by virtual servers
    #lb_service = <None>

    # Resource ID of the IPSet containing the IPs of all the virtual servers
    #lb_vs_ip_set = <None>

    # Enable X_forward_for for ingress. Available values are INSERT or REPLACE.
    # When this config is set, if x_forwarded_for is missing, LB will add
    # x_forwarded_for in the request header with value client ip. When
    # x_forwarded_for is present and its set to REPLACE, LB will replace
    # x_forwarded_for in the header to client_ip. When x_forwarded_for is
    # present and its set to INSERT, LB will append client_ip to
    # x_forwarded_for in the header. If not wanting to use x_forwarded_for,
    # remove this config
    # Choices: <None> INSERT REPLACE
    #x_forwarded_for = <None>


    # Resource ID of the firewall section that will be used to create firewall
    # sections below this mark section
    #top_firewall_section_marker = <None>

    # Resource ID of the firewall section that will be used to create firewall
    # sections above this mark section
    #bottom_firewall_section_marker = <None>

    # Replication mode of container logical switch, set SOURCE for cloud as it
    # only supports head replication mode
    # Choices: MTEP SOURCE
    #ls_replication_mode = MTEP



    # The resource which NCP will search tag 'node_name' on, to get parent VIF
    # or transport node uuid for container LSP API context field. For HOSTVM
    # mode, it will search tag on LSP. For BM mode, it will search tag on LSP
    # then search TN. For CLOUD mode, it will search tag on VM. For WCP_WORKER
    # mode, it will search TN by hostname.
    # Choices: tag_on_lsp tag_on_tn tag_on_vm hostname_on_tn
    #search_node_tag_on = tag_on_lsp

    # Determines which kind of information to be used as VIF app_id. Defaults
    # to pod_resource_key. In WCP mode, pod_uid is used.
    # Choices: pod_resource_key pod_uid
    #vif_app_id_type = pod_resource_key


    # If this value is not empty, NCP will append it to nameserver list
    #dns_servers = []

    # Set this to True to enable NCP to report errors through NSXError CRD.
    #enable_nsx_err_crd = False

    # Maximum number of virtual servers allowed to create in cluster for
    # LoadBalancer type of services.
    #max_allowed_virtual_servers = 9223372036854775807

    # Edge cluster ID needed when creating Tier1 router for loadbalancer
    # service. Information could be retrieved from Tier0 router
    edge_cluster = a4ef76aa-e366-461b-8488-75f2f9796548

    # Inventory feature switch
    #enable_inventory = True





    # For internal container network CIDR, NCP adds redistribution deny rule to
    # stop T0 router advertise subnets to external network outside of T0
    # router. If BGP or route redistribution is disabled, or
    # T1_CONNECTED/TIER1_SEGMENT option is not selected, NCP would not add the
    # deny rule. If users enable BGP and route redistribution, or select
    # T1_CONNECTED/TIER1_SEGMENT option after NCP starts, user needs to restart
    # NCP to let NCP set deny rule. If there is already a route map attached,
    # NCP will create IP prefix list on the existing route map. Otherwise NCP
    # will create a new route map and attach it. This option could be used only
    # in SNAT mode and when policy_nsxapi = True.
    #configure_t0_redistribution = False


    # Health check interval for nsx lb monitor profile
    #lb_hc_profile_interval = 5

    # Health check timeout for nsx lb monitor profile
    #lb_hc_profile_timeout = 15

    # Health check failed count for nsx lb monitor profile. Pool member failed
    # for this amount will be marked as down.
    #lb_hc_profile_fall_count = 3

    # Health check rise count for nsx lb monitor profile. Pool members
    # previously marked down will be brought up, if succeed in the health check
    # for this amount fo time.
    #lb_hc_profile_rise_count = 3

    # Maximum size of the buffer used to store HTTP request headers for L7
    # virtual servers in cluster. A request with header larger than this value
    # will be processed as best effort whereas a request with header below this
    # value is guaranteed to be processed.
    #lb_http_request_header_size = 1024

    # Maximum size of the buffer used to store HTTP response headers for all L7
    # virtual servers in cluster. A response with header larger than this value
    # will be dropped.
    #lb_http_response_header_size = 4096

    # Maximum server idle time in seconds for L7 virtual servers in cluster. If
    # backend server does not send any packet within this time, the connection
    # is closed.
    #lb_http_response_timeout = 60

    # Determines the behavior when a Tier-1 instance restarts after a failure.
    # If set to PREEMPTIVE, the preferred node will take over, even if it
    # causes another failure. If set to NON_PREEMPTIVE, then the instance that
    # restarted will remain secondary. Applicable to Tier-1 across cluster that
    # was created by NCP and has edge cluster configured.
    # Choices: PREEMPTIVE NON_PREEMPTIVE
    #failover_mode = NON_PREEMPTIVE

    # Set this to ENABLE to enable NCP enforced pool member limit for all load
    # balancer servers in cluster. Set this to CRD_LB_ONLY will only enforce
    # the limit for load balancer servers created using lb CRD. Set this to
    # DISABLE to turn off all limit checks. This option requires
    # relax_scale_validation set to True, l4_lb_auto_scaling set to False, and
    # works on Policy API only. When not disabled, NCP will enforce a pool
    # member limit on LBS to prevent one LBS from using up all resources on
    # edge nodes.
    # Choices: DISABLE ENABLE CRD_LB_ONLY
    #ncp_enforced_pool_member_limit = DISABLE

    # Maximum number of pool member allowed for each small load balancer
    # service. Requires ncp_enforced_pool_member_limit set to ENABLE or
    # CRD_LB_ONLY to take effect.
    #members_per_small_lbs = 2000

    # Maximum number of pool member allowed for each medium load balancer
    # service. Requires ncp_enforced_pool_member_limit set to ENABLE or
    # CRD_LB_ONLY to take effect.
    #members_per_medium_lbs = 2000

    # Interval in seconds to clean empty segments.
    #segment_gc_interval = 600

    # Determines the mode NCP limits rate when sending API calls to NSX.
    # Choices: NO_LIMIT SLIDING_WINDOW ADAPTIVE_AIMD
    #api_rate_limit_mode = ADAPTIVE_AIMD

    # When nsx_v3.api_rate_limit_mode is not set to NO_LIMIT, determines the
    # maximum number of API calls sent per manager ip per second. Should be a
    # positive integer.
    #max_api_rate = 40

    # Resource ID of the client SSL profile which will be used by Loadbalancer
    # while participating in TLS handshake with the client
    #client_ssl_profile = <None>

    # Enable security policy notification, If this optionis enabled, NCP will
    # configure container network afterNSX creates logical port and finishes
    # security policysynchronization
    #wait_for_security_policy_sync = False


    # Set this to True to enable rule tag as cluster name in DFW logs for k8s
    #enable_rule_tag = True


    [ha]


    # Time duration in seconds of mastership timeout. NCP instance will remain
    # master for this duration after elected. Note that the heartbeat period
    # plus the update timeout must not be greater than this period. This is
    # done to ensure that the master instance will either confirm liveness or
    # fail before the timeout.
    #master_timeout = 18

    # Time in seconds between heartbeats for elected leader. Once an NCP
    # instance is elected master, it will periodically confirm liveness based
    # on this value.
    #heartbeat_period = 6

    # Timeout duration in seconds for update to election resource. The default
    # value is calculated by subtracting heartbeat period from master timeout.
    # If the update request does not complete before the timeout it will be
    # aborted. Used for master heartbeats to ensure that the update finishes or
    # is aborted before the master timeout occurs.
    #update_timeout = <None>


    [k8s]

    # Kubernetes API server IP address.
    apiserver_host_ip = 192.168.10.11

    # Kubernetes API server port.
    apiserver_host_port = 6443

    # Full path of the Token file to use for authenticating with the k8s API
    # server.
    client_token_file = /var/run/secrets/kubernetes.io/serviceaccount/token

    # Full path of the client certificate file to use for authenticating with
    # the k8s API server. It must be specified together with
    # "client_private_key_file".
    #client_cert_file = <None>

    # Full path of the client private key file to use for authenticating with
    # the k8s API server. It must be specified together with
    # "client_cert_file".
    #client_private_key_file = <None>

    # Specify a CA bundle file to use in verifying the k8s API server
    # certificate.
    ca_file = /var/run/secrets/kubernetes.io/serviceaccount/ca.crt

    # Specify whether ingress controllers are expected to be deployed in
    # hostnework mode or as regular pods externally accessed via NAT
    # Choices: hostnetwork nat
    ingress_mode = nat

    # Log level for the kubernetes adaptor. Ignored if debug is True
    # Choices: NOTSET DEBUG INFO WARNING ERROR CRITICAL
    #loglevel = <None>

    # The default HTTP ingress port for non-NSX ingress controllers in NAT
    # mode.
    #http_ingress_port = 80

    # The default HTTPS ingress port for non-NSX ingress controllers in NAT
    # mode.
    #https_ingress_port = 443


    # Specify thread pool size to process resource events
    #resource_watcher_thread_pool_size = 1

    # User specified IP address for HTTP and HTTPS ingresses
    #http_and_https_ingress_ip = <None>

    # Set this to True to enable NCP to create tier1 router, first segment and
    # default SNAT IP for VirtualNetwork CRD, and then create segment port for
    # VM through VirtualNetworkInterface CRD.
    #enable_vnet_crd = False

    # Set this to True to enable NCP to create LoadBalancer on a Tier-1 for
    # LoadBalancer CRD. This option does not support LB autoscaling.
    #enable_lb_crd = False

    # Option to set the type of baseline cluster policy. ALLOW_CLUSTER creates
    # an explicit baseline policy to allow any pod to communicate any other pod
    # within the cluster. ALLOW_NAMESPACE creates an explicit baseline policy
    # to allow pods within the same namespace to communicate with each other.
    # By default, no baseline rule will be created and the cluster will assume
    # the default behavior as specified by the backend. Modification is not
    # supported after the value is set.
    # Choices: <None> allow_cluster allow_namespace
    baseline_policy_type = allow_namespace

    # Maximum number of endpoints allowed to create for a service.
    #max_allowed_endpoints = 1000

    # Set this to True to enable NCP reporting NSX backend error to k8s object
    # using k8s event
    #enable_ncp_event = False

    # Set this to True to enable multus to create multiple interfaces for one
    # pod. Requires policy_nsxapi set to True to take effect. If passthrough
    # interface is used as additional interface, user should deploy device
    # plugin to provide device allocation information for NCP.Pod annotations
    # under prefix "k8s.v1.cni.cncf.io" cannot be modified after pod realized.
    # User defined IP will not be allocated from Segment IPPool. "gateway" in
    # NetworkAttachmentDefinition is not used to configure secondary
    # interfaces. Default gateway of pod is configured by primary interface.
    # User must define IP and/or MAC if no "ipam" is configured.Only available
    # if node type is HOSTVM
    #enable_multus = True


    # Interval of polling loadbalancer statistics. Default to60 seconds.
    #lb_statistic_monitor_interval = 60

    # This option is for toggling process of network CRD.It should be set to
    # False when the network status setting is done by OCP4 NetworkOperator
    #process_oc_network = True



    #[nsx_v3]
    # Deprecated option: tier0_router
    # Replaced by [nsx_v3] top_tier_router

    # Deprecated option: deny_subnets_redistribution
    # Replaced by [nsx_v3] configure_t0_redistribution


---
# ConfigMap for VERSION
apiVersion: v1
kind: ConfigMap
metadata:
  name: nsx-ncp-version-config
  namespace: nsx-system
  labels:
    version: v1
data:
  version: 3.1.0
---
apiVersion: apps/v1
kind: Deployment
metadata:
  # VMware NSX Container Plugin
  name: nsx-ncp
  namespace: nsx-system
  labels:
    tier: nsx-networking
    component: nsx-ncp
    version: v1
spec:
  # Active-Standby is supported from NCP 2.4.0 release,
  # so replica can be more than 1 if NCP HA is enabled.
  # replica *must be* 1 if NCP HA is disabled.
  selector:
    matchLabels:
      tier: nsx-networking
      component: nsx-ncp
      version: v1
  replicas: 1
  template:
    metadata:
      labels:
        tier: nsx-networking
        component: nsx-ncp
        version: v1
    spec:
      # NCP shares the host management network.
      hostNetwork: true
      tolerations:
        - key: node-role.kubernetes.io/master
          effect: NoSchedule
      # If configured with ServiceAccount, update the ServiceAccount
      # name below.
      serviceAccountName: ncp-svc-account
      # podAntiAffinity could ensure that NCP replicas are not be co-located
      # on a single node
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: component
                  operator: In
                  values:
                  - nsx-ncp
                - key: tier
                  operator: In
                  values:
                  - nsx-networking
              topologyKey: "kubernetes.io/hostname"

      containers:
        - name: nsx-ncp
          # Docker image for NCP
          image: nnikodimov/nsx-ncp-ubuntu:3.1.0
          imagePullPolicy: IfNotPresent
          env:
            - name: NCP_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: NCP_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          livenessProbe:
            exec:
              command:
                - /bin/sh
                - -c
                - check_pod_liveness nsx-ncp 5
            initialDelaySeconds: 5
            timeoutSeconds: 5
            periodSeconds: 10
            failureThreshold: 5
          volumeMounts:
          - name: projected-volume
            mountPath: /etc/nsx-ujo
            readOnly: true

      volumes:

        - name: projected-volume
          projected:
            sources:
              # ConfigMap nsx-ncp-config is expected to supply ncp.ini
              - configMap:
                  name: nsx-ncp-config
                  items:
                    - key: ncp.ini
                      path: ncp.ini
              # ConfigMap nsx-ncp-version-config is expected to supply VERSION
              - configMap:
                  name: nsx-ncp-version-config
                  items:
                    - key: version
                      path: VERSION
              # To use cert based auth, uncomment and update the secretName,
              # then update ncp.ini with the mounted cert and key file paths
              #- secret:
              #    name: nsx-secret
              #    items:
              #      - key: tls.crt
              #        path: nsx-cert/tls.crt
              #      - key: tls.key
              #        path: nsx-cert/tls.key

              - secret:
                  name: lb-secret
                  items:
                    - key: tls.crt
                      path: lb-cert/tls.crt
                    - key: tls.key
                      path: lb-cert/tls.key

              # To use JWT based auth, uncomment and update the secretName.
              #- secret:
              #    name: wcp-cluster-credentials
              #    items:
              #      - key: username
              #        path: vc/username
              #      - key: password
              #        path: vc/password


---
# Yaml template for nsx-node-agent and nsx-kube-proxy DaemonSet
# Proper kubernetes API parameters and NCP Docker image must be
# specified.
# This yaml file is part of NCP 3.1.0 release.

# ConfigMap for ncp.ini
apiVersion: v1
kind: ConfigMap
metadata:
  name: nsx-node-agent-config
  namespace: nsx-system
  labels:
    version: v1
data:
  ncp.ini: |
    

    [DEFAULT]

    # If set to true, the logging level will be set to DEBUG instead of the
    # default INFO level.
    #debug = False

    # If set to true, log output to standard error.
    #use_stderr = True

    # Destination to send api log to. STDOUT or STDERR for console output. FILE
    # to write log to file configured in "api_log_file". NONE to disable api
    # log.
    # Choices: STDOUT STDERR FILE NONE
    #api_log_output = NONE

    # Name of log file to send API access log to.
    #api_log_file = ncp_api_log.txt

    # Interval in seconds to logs api call to output configured in
    # api_log_output
    #api_log_interval = 60

    # When api_log_output is not NONE, this option determines if api calls
    # should be collected per NSX cluster or individual NSX manager.
    # Choices: API_LOG_PER_ENDPOINT API_LOG_PER_CLUSTER
    #api_log_mode = API_LOG_PER_ENDPOINT

    # If set to true, use syslog for logging.
    #use_syslog = False

    # The base directory used for relative log_file paths.
    #log_dir = <None>

    # Name of log file to send logging output to.
    #log_file = <None>

    # max MB for each compressed file. Defaults to 100 MB.
    #log_rotation_file_max_mb = 100

    # max MB for each compressed file for API logs.Defaults to 10 MB.
    #api_log_rotation_file_max_mb = 10

    # Total number of compressed backup files to store. Defaults to 5.
    #log_rotation_backup_count = 5

    # Total number of compressed backup files to store API logs. Defaults to 5.
    #api_log_rotation_backup_count = 5

    # Log level for the root logger. If debug=True, the default root logger
    # level will be DEBUG regardless of the value of this option. If this
    # option is unset, the default root logger level will be either DEBUG or
    # INFO according to the debug option value
    # Choices: NOTSET DEBUG INFO WARNING ERROR CRITICAL
    #loglevel = <None>




    [k8s]

    # Kubernetes API server IP address.
    apiserver_host_ip = 192.168.10.11

    # Kubernetes API server port.
    apiserver_host_port = 6443

    # Full path of the Token file to use for authenticating with the k8s API
    # server.
    client_token_file = /var/run/secrets/kubernetes.io/serviceaccount/token

    # Full path of the client certificate file to use for authenticating with
    # the k8s API server. It must be specified together with
    # "client_private_key_file".
    #client_cert_file = <None>

    # Full path of the client private key file to use for authenticating with
    # the k8s API server. It must be specified together with
    # "client_cert_file".
    #client_private_key_file = <None>

    # Specify a CA bundle file to use in verifying the k8s API server
    # certificate.
    ca_file = /var/run/secrets/kubernetes.io/serviceaccount/ca.crt

    # Specify whether ingress controllers are expected to be deployed in
    # hostnework mode or as regular pods externally accessed via NAT
    # Choices: hostnetwork nat
    ingress_mode = nat

    # Log level for the kubernetes adaptor. Ignored if debug is True
    # Choices: NOTSET DEBUG INFO WARNING ERROR CRITICAL
    #loglevel = <None>

    # The default HTTP ingress port for non-NSX ingress controllers in NAT
    # mode.
    #http_ingress_port = 80

    # The default HTTPS ingress port for non-NSX ingress controllers in NAT
    # mode.
    #https_ingress_port = 443


    # Specify thread pool size to process resource events
    #resource_watcher_thread_pool_size = 1

    # User specified IP address for HTTP and HTTPS ingresses
    #http_and_https_ingress_ip = <None>

    # Set this to True to enable NCP to create tier1 router, first segment and
    # default SNAT IP for VirtualNetwork CRD, and then create segment port for
    # VM through VirtualNetworkInterface CRD.
    #enable_vnet_crd = False

    # Set this to True to enable NCP to create LoadBalancer on a Tier-1 for
    # LoadBalancer CRD. This option does not support LB autoscaling.
    #enable_lb_crd = False

    # Option to set the type of baseline cluster policy. ALLOW_CLUSTER creates
    # an explicit baseline policy to allow any pod to communicate any other pod
    # within the cluster. ALLOW_NAMESPACE creates an explicit baseline policy
    # to allow pods within the same namespace to communicate with each other.
    # By default, no baseline rule will be created and the cluster will assume
    # the default behavior as specified by the backend. Modification is not
    # supported after the value is set.
    # Choices: <None> allow_cluster allow_namespace
    baseline_policy_type = allow_namespace

    # Maximum number of endpoints allowed to create for a service.
    #max_allowed_endpoints = 1000

    # Set this to True to enable NCP reporting NSX backend error to k8s object
    # using k8s event
    #enable_ncp_event = False

    # Set this to True to enable multus to create multiple interfaces for one
    # pod. Requires policy_nsxapi set to True to take effect. If passthrough
    # interface is used as additional interface, user should deploy device
    # plugin to provide device allocation information for NCP.Pod annotations
    # under prefix "k8s.v1.cni.cncf.io" cannot be modified after pod realized.
    # User defined IP will not be allocated from Segment IPPool. "gateway" in
    # NetworkAttachmentDefinition is not used to configure secondary
    # interfaces. Default gateway of pod is configured by primary interface.
    # User must define IP and/or MAC if no "ipam" is configured.Only available
    # if node type is HOSTVM
    #enable_multus = True


    # Interval of polling loadbalancer statistics. Default to60 seconds.
    #lb_statistic_monitor_interval = 60

    # This option is for toggling process of network CRD.It should be set to
    # False when the network status setting is done by OCP4 NetworkOperator
    #process_oc_network = True


    [coe]

    # Container orchestrator adaptor to plug in.
    adaptor = kubernetes

    # Specify cluster for adaptor.
    cluster = sdc-tkg-service

    # Log level for NCP modules (controllers, services, etc.). Ignored if debug
    # is True
    # Choices: NOTSET DEBUG INFO WARNING ERROR CRITICAL
    #loglevel = <None>

    # Log level for NSX API client operations. Ignored if debug is True
    # Choices: NOTSET DEBUG INFO WARNING ERROR CRITICAL
    #nsxlib_loglevel = <None>

    # Enable SNAT for all projects in this cluster. Modification of topologies
    # for existing Namespaces is not supported if this option is reset.
    enable_snat = True

    # Option to enable profiling
    #profiling = False

    # The interval of reporting performance metrics (0 means disabled)
    #metrics_interval = 0

    # Name of log file for outputting metrics only (if not defined, use default
    # logging facility)
    #metrics_log_file = <None>

    # The type of container host node
    # Choices: HOSTVM BAREMETAL CLOUD WCP_WORKER
    #node_type = HOSTVM

    # The time in seconds for NCP/nsx_node_agent to recover the connection to
    # NSX manager/container orchestrator adaptor/Hyperbus before exiting. If
    # the value is 0, NCP/nsx_node_agent won't exit automatically when the
    # connection check fails
    #connect_retry_timeout = 0


    # Enable system health status report for SHA
    #enable_sha = True


    [nsx_kube_proxy]

    # The way to process service configuration, set into OVS flow or write to
    # nestdb,
    # Choices: ovs nestdb
    #config_handler = ovs




    [nsx_node_agent]

    # Prefix of node /proc and /var/run/netns path to mount on nsx_node_agent
    # DaemonSet
    #proc_mount_path_prefix = /host




    # The log level of NSX RPC library. Ignored if debug is True
    # Choices: NOTSET DEBUG INFO WARNING ERROR CRITICAL
    #nsxrpc_loglevel = ERROR

    # OVS bridge name
    #ovs_bridge = br-int

    # The time in seconds for nsx_node_agent to wait CIF config from HyperBus
    # before returning to CNI
    #config_retry_timeout = 300

    # The time in seconds for nsx_node_agent to backoff before re-using an
    # existing cached CIF to serve CNI request. Must be less than
    # config_retry_timeout.
    #config_reuse_backoff_time = 15


    # The OVS uplink OpenFlow port where to apply the NAT rules to.
    ovs_uplink_port = eth0

    # Set this to True if you want to install and use the NSX-OVS kernel
    # module. If the host OS is supported, it will be installed by nsx-ncp-
    # bootstrap and used by nsx-ovs container in nsx-node-agent pod. Note that
    # you would have to add (uncomment) the volumes and mounts in the nsx-ncp-
    # bootstrap DS and add SYS_MODULE capability in nsx-ovs container spec in
    # nsx-node-agent DS. Failing to do so will result in failure of
    # installation and/or kernel upgrade of NSX-OVS kernelmodule.
    #use_nsx_ovs_kernel_module = False

    # The time in seconds for nsx_node_agent to call OVS command. Please
    # increase the time if OVS is in heavy load to create/delete ports
    #ovs_operation_timeout = 5

    # Set to true to allow the CNI plugin to enable IPv6 container interfaces
    #enable_ipv6 = False

    # Set to True if DHCP is configured on the "ovs_uplink_port". "auto" will
    # try to automatically  infer it but it only works on CoreOS. On other
    # types host OS, it defaults to False
    # Choices: True False auto
    #is_dhcp_configured_on_ovs_uplink_port = auto

    # The MTU value for nsx-cni
    #mtu = 1500

    # Applicable only in PKS. If set, nsx-node-agent watches for addition,
    # removal, and update of nodelocaldns DaemonSet. Upon a change, it
    # terminates and is restarted by the monit agent
    #enable_nodelocaldns_monitoring = False

    # The waiting time before nsx-node-agent returns response to CNI plugin,
    # there is a potential timing issue between port creation and related
    # firewall config update on Hypervisor host
    #waiting_before_cni_response = 0



---
# nsx-ncp-bootstrap DaemonSet
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: nsx-ncp-bootstrap
  namespace: nsx-system
  labels:
    tier: nsx-networking
    component: nsx-ncp-bootstrap
    version: v1
spec:
  selector:
    matchLabels:
      tier: nsx-networking
      component: nsx-ncp-bootstrap
      version: v1
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        tier: nsx-networking
        component: nsx-ncp-bootstrap
        version: v1
    spec:
      hostNetwork: true
      tolerations:
        - key: node-role.kubernetes.io/master
          effect: NoSchedule
        - key: node.kubernetes.io/not-ready
          effect: NoSchedule
        - key: node.kubernetes.io/unreachable
          effect: NoSchedule
      hostPID: true
      # If configured with ServiceAccount, update the ServiceAccount
      # name below.
      serviceAccountName: nsx-node-agent-svc-account
      initContainers:
        - name: nsx-ncp-bootstrap
          # Docker image for NCP
          image: nnikodimov/nsx-ncp-ubuntu:3.1.0
          imagePullPolicy: IfNotPresent
          # override NCP image entrypoint
          command: ["init_k8s_node"]

          securityContext:
            # privilege mode required to load apparmor on ubuntu
            privileged: true
            runAsUser: 0


          volumeMounts:
          # required to read the ovs_uplink_port
          - name: projected-volume
            mountPath: /etc/nsx-ujo
          # mounts to which NSX-CNI are copied BEGIN
          - name: host-etc
            mountPath: /host/etc
          - name: host-opt
            mountPath: /host/opt
          - name: host-var
            mountPath: /host/var
          # mounts to which NSX-CNI are copied END
          # mount host's OS info to identify host OS
          - name: host-os-release
            mountPath: /host/etc/os-release
          # mount ovs runtime files to stop running OVS kmod
          - name: openvswitch
            mountPath: /var/run/openvswitch
          # Uncomment these mounts if installing NSX-OVS kernel module
        #   # mount host lib modules to install OVS kernel module if needed
        #   - name: host-modules
        #     mountPath: /lib/modules
        #   # mount openvswitch database
        #   - name: host-config-openvswitch
        #     mountPath: /etc/openvswitch
        #   - name: dir-tmp-usr-ovs-kmod-backup
        #   # we move the usr kmod files to this dir temporarily before
        #   # installing new OVS kmod and/or backing up existing OVS kmod backup
        #     mountPath: /tmp/nsx_usr_ovs_kmod_backup

        #   # mount linux headers for compiling OVS kmod
        #   - name: host-usr-src
        #     mountPath: /usr/src
          # mount apparmor files to load the node-agent-apparmor
          - name: app-armor-cache
            mountPath: /var/cache/apparmor
            subPath: apparmor
          - name: apparmor-d
            mountPath: /etc/apparmor.d
          # mount host's deb package database to remove nsx-cni if installed
          - name: dpkg-lib
            mountPath: /host/var/lib/dpkg
          # mount for uninstalling NSX-CNI old doc
          - name: usr-share-doc
            mountPath: /usr/share/doc
          - name: snap-confine
            mountPath: /var/lib/snapd/apparmor/snap-confine


      containers:
        - name: nsx-dummy
          # This container is of no use.
          # Docker image for NCP
          image: nnikodimov/nsx-ncp-ubuntu:3.1.0
          imagePullPolicy: IfNotPresent
          # override NCP image entrypoint
          command: ["/bin/bash", "-c", "while true; do sleep 5; done"]
      volumes:
        - name: projected-volume
          projected:
            sources:
              - configMap:
                  name: nsx-node-agent-config
                  items:
                    - key: ncp.ini
                      path: ncp.ini
        - name: host-etc
          hostPath:
            path: /etc
        - name: host-opt
          hostPath:
            path: /opt
        - name: host-var
          hostPath:
            path: /var
        - name: host-os-release
          hostPath:
            path: /etc/os-release
        - name: openvswitch
          hostPath:
            path: /var/run/openvswitch
        # Uncomment these volumes if installing NSX-OVS kernel module
        # - name: host-modules
        #   hostPath:
        #     path: /lib/modules
        # - name: host-config-openvswitch
        #   hostPath:
        #     path: /etc/openvswitch
        # - name: dir-tmp-usr-ovs-kmod-backup
        #   hostPath:
        #     path: /tmp/nsx_usr_ovs_kmod_backup

        # - name: host-usr-src
        #   hostPath:
        #     path: /usr/src
        - name: app-armor-cache
          hostPath:
            path: /var/cache/apparmor
        - name: apparmor-d
          hostPath:
            path: /etc/apparmor.d
        - name: dpkg-lib
          hostPath:
            path: /var/lib/dpkg
        - name: usr-share-doc
          hostPath:
            path: /usr/share/doc
        - name: snap-confine
          hostPath:
            path: /var/lib/snapd/apparmor/snap-confine


---
# nsx-node-agent DaemonSet
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: nsx-node-agent
  namespace: nsx-system
  labels:
    tier: nsx-networking
    component: nsx-node-agent
    version: v1
spec:
  selector:
    matchLabels:
      tier: nsx-networking
      component: nsx-node-agent
      version: v1
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:

      annotations:
        container.apparmor.security.beta.kubernetes.io/nsx-node-agent: localhost/node-agent-apparmor

      labels:
        tier: nsx-networking
        component: nsx-node-agent
        version: v1
    spec:
      hostNetwork: true
      # Give enough time to save flows and perform stop_ovs
      terminationGracePeriodSeconds: 60
      tolerations:
        - key: node-role.kubernetes.io/master
          effect: NoSchedule
        - key: node.kubernetes.io/not-ready
          effect: NoSchedule
        - key: node.kubernetes.io/unreachable
          effect: NoSchedule
      # If configured with ServiceAccount, update the ServiceAccount
      # name below.
      serviceAccountName: nsx-node-agent-svc-account
      containers:

        - name: nsx-node-agent
          # Docker image for NCP
          image: nnikodimov/nsx-ncp-ubuntu:3.1.0
          imagePullPolicy: IfNotPresent
          # override NCP image entrypoint
          command: ["start_node_agent"]
          env:
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: CONTAINER_NAME
              value: nsx-node-agent
          livenessProbe:
            exec:
              command:
                - /bin/sh
                - -c
                - check_pod_liveness nsx-node-agent 5
            initialDelaySeconds: 60
            timeoutSeconds: 5
            periodSeconds: 10
            failureThreshold: 5
          securityContext:
            capabilities:
              add:
                - NET_ADMIN
                - SYS_ADMIN
                - SYS_PTRACE
                - DAC_READ_SEARCH
          volumeMounts:
          # ncp.ini
          - name: projected-volume
            mountPath: /etc/nsx-ujo
            readOnly: true
          # mount openvswitch dir
          - name: openvswitch
            mountPath: /var/run/openvswitch
          # mount CNI socket path
          - name: var-run-ujo
            mountPath: /var/run/nsx-ujo
          # mount container namespace
          - name: netns
            mountPath: /host/var/run/netns
            # for containerd support
            mountPropagation: HostToContainer
          # mount host proc
          - name: proc
            mountPath: /host/proc
            readOnly: true
          # mount kubelet device plugin data dir
          - name: device-plugins
            mountPath: /var/lib/kubelet/device-plugins/
            readOnly: true

        - name: nsx-kube-proxy
          # Docker image for NCP
          image: nnikodimov/nsx-ncp-ubuntu:3.1.0
          imagePullPolicy: IfNotPresent
          # override NCP image entrypoint
          command: ["start_kube_proxy"]
          env:
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: CONTAINER_NAME
              value: nsx-kube-proxy
          livenessProbe:
            exec:
              command:
                - /bin/sh
                - -c
                - check_pod_liveness nsx-kube-proxy 5
            initialDelaySeconds: 10
            timeoutSeconds: 5
            periodSeconds: 10
            failureThreshold: 5
          securityContext:
            capabilities:
              add:
                - NET_ADMIN
                - SYS_ADMIN
                - SYS_PTRACE
                - DAC_READ_SEARCH

          volumeMounts:
          # ncp.ini
          - name: projected-volume
            mountPath: /etc/nsx-ujo
            readOnly: true

          # mount openvswitch dir
          - name: openvswitch
            mountPath: /var/run/openvswitch


        # nsx-ovs is not needed on BAREMETAL
        - name: nsx-ovs
          # Docker image for NCP
          image: nnikodimov/nsx-ncp-ubuntu:3.1.0
          imagePullPolicy: IfNotPresent
          # override NCP image entrypoint
          command: ["start_ovs"]
          securityContext:
            capabilities:
              add:
                - NET_ADMIN
                - SYS_ADMIN
                - SYS_NICE
                - SYS_MODULE
          livenessProbe:
            exec:
              command:
                - /bin/sh
                - -c
                # You must pass --allowOVSOnHost if you are running OVS on the
                # host before the installation. This allows livenessProbe to
                # succeed and container won't restart frequently.
                - check_pod_liveness nsx-ovs 10
            initialDelaySeconds: 10
            periodSeconds: 10
            timeoutSeconds: 10
          volumeMounts:
          # ncp.ini
          - name: projected-volume
            mountPath: /etc/nsx-ujo
            readOnly: true
          # mount openvswitch-db dir
          - name: var-run-ujo
            mountPath: /etc/openvswitch
            subPath: openvswitch-db
          # mount openvswitch dir
          - name: openvswitch
            mountPath: /var/run/openvswitch
          # mount host sys dir
          - name: host-sys
            mountPath: /sys
            readOnly: true
          # mount host config openvswitch dir
          - name: host-original-ovs-db
            mountPath: /host/etc/openvswitch
          # mount host lib modules to insert OVS kernel module if needed
          - name: host-modules
            mountPath: /lib/modules
            readOnly: true
          # mount host's OS info to identify host OS
          - name: host-os-release
            mountPath: /host/etc/os-release
            readOnly: true
          # OVS puts logs into this mountPath by default
          - name: host-var-log-ujo
            mountPath: /var/log/openvswitch
            subPath: openvswitch


      volumes:
        - name: projected-volume
          projected:
            sources:
              - configMap:
                  name: nsx-node-agent-config
                  items:
                    - key: ncp.ini
                      path: ncp.ini
              - configMap:
                  name: nsx-ncp-version-config
                  items:
                    - key: version
                      path: VERSION

        - name: openvswitch
          hostPath:
            path: /var/run/openvswitch
        - name: var-run-ujo
          hostPath:
            path: /var/run/nsx-ujo
        - name: netns
          hostPath:
            path: /var/run/netns
        - name: proc
          hostPath:
            path: /proc
        - name: device-plugins
          hostPath:
            path: /var/lib/kubelet/device-plugins/


        - name: host-sys
          hostPath:
            path: /sys
        - name: host-modules
          hostPath:
            path: /lib/modules
        # This is the directory where OVS that runs on the host stores
        # its conf.db file. OVS uses this directory by default but if
        # you had a it configured differently, please update it here
        - name: host-original-ovs-db
          hostPath:
            path: /etc/openvswitch
        - name: host-os-release
          hostPath:
            path: /etc/os-release
        - name: host-var-log-ujo
          hostPath:
            path: /var/log/nsx-ujo
            type: DirectoryOrCreate



